{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urllib.parse.urlencode(\"http://google.com/q=this is a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "def parse_json_file_to_csv(fname): \n",
    "    keyz = [ 'prism:url', \n",
    "             \"dc:identifier\", \"dc:title\", \"prism:publicationName\",\n",
    "             \"prism:coverDate\", \"citedby-count\", \n",
    "            ]\n",
    "    affilz = \"affiliation\" ## sub = affilname, affiliation-city, affiliation-country\n",
    "    first_affilz_country = \"country_1\"\n",
    "    first_affilz_org = \"institution_1\"\n",
    "    \n",
    "    has_fundus_in_title = \"fundus_in_title\"\n",
    "    pub_year = \"pub_year\"\n",
    "    \n",
    "    abhref = 'abstract'\n",
    "    \n",
    "    def parse_elsevier_json_item(item):    \n",
    "        \n",
    "        def get_pub_year(pub_date):\n",
    "            return datetime.strptime(pub_date, \"%Y-%m-%d\").year\n",
    "        \n",
    "        def has_fundus(titl):\n",
    "            return str( int( 'fundus' in titl.lower() ) )\n",
    "        \n",
    "        O_ = []\n",
    "        ## 1. everything top level \n",
    "        for k in keyz:\n",
    "            O_.append( item[k] ) \n",
    "        \n",
    "        ## 2. parse affiliation details and join org,city,country\n",
    "        k_affz = ['affilname', 'affiliation-city', 'affiliation-country']\n",
    "        A_ = []\n",
    "        I_ = '-'\n",
    "        C_ = '-'\n",
    "        ia = 0 \n",
    "        affitem = item.get(\"affiliation\", None)\n",
    "        if affitem:\n",
    "            for aff in affitem:\n",
    "                if aff:\n",
    "                    a_ = []\n",
    "                    for k in k_affz:\n",
    "                        x = aff.get(k, \"-\") \n",
    "                        a_.append( x if x is not None else \"-\")\n",
    "#                     print( a_ )\n",
    "                    A_.append(\", \".join(a_) )         \n",
    "                    if ia == 0:\n",
    "                        C_ = a_[-1]  \n",
    "                        I_ = a_[0]\n",
    "                        ia = 100 \n",
    "            O_.append( \"++\".join(A_) )\n",
    "        else:\n",
    "            O_.append( \"-\" )\n",
    "        O_.append( I_ )\n",
    "        O_.append( C_ )\n",
    "        \n",
    "        ## 3. preprocs\n",
    "        O_.append( has_fundus( item.get(\"dc:title\", \"\") ) )\n",
    "        O_.append( get_pub_year(item.get(\"prism:coverDate\", \"\")))\n",
    "        \n",
    "        ## 4. parse abstract url \n",
    "        abstract = \"link\" # @ref = \"self\", \"@href\" \n",
    "        for abl in item[abstract]:\n",
    "            if abl['@ref'] == 'self':\n",
    "                O_.append( abl['@href'] ) \n",
    "        \n",
    "        return O_ \n",
    "    \n",
    "        \n",
    "    def fetch_abstract(ahref):\n",
    "        pass \n",
    "    \n",
    "    to_csv = []\n",
    "    with open( f'{fname}.json', 'r') as fd:\n",
    "        pagez = json.load( fd)\n",
    "#         print( type(pagez), len(pagez) )\n",
    "        for page in pagez:\n",
    "            for item in page:\n",
    "                to_csv.append( parse_elsevier_json_item(item) )\n",
    "            \n",
    "    \n",
    "    def dump_csv_row(rec):\n",
    "#         print(type(rec), len(rec), rec)\n",
    "        fd.write( \"\\t\".join( rec) )\n",
    "        fd.write(\"\\n\")\n",
    "        \n",
    "    with open( f\"{fname}.csv\", 'w') as fd:\n",
    "        headz = keyz + [affilz, first_affilz_org, first_affilz_country, \n",
    "                        has_fundus_in_title, pub_year, \n",
    "                        abhref] \n",
    "        dump_csv_row(headz)\n",
    "        for rec in to_csv:\n",
    "            dump_csv_row(rec)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 9: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-33b8bfe82725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mparse_json_file_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"******** FINISHED - \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-2c11e128f2c8>\u001b[0m in \u001b[0;36mparse_json_file_to_csv\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdump_csv_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_csv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mdump_csv_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-2c11e128f2c8>\u001b[0m in \u001b[0;36mdump_csv_row\u001b[0;34m(rec)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump_csv_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#         print(type(rec), len(rec), rec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 9: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "datz = ['0ea_4__fuse_hc_dl', \n",
    "        '0ea_5__fundus_hc_dl', \n",
    "        '0ea_642__medical_img_preproc', \n",
    "        '0ea_3926__fundus'\n",
    "       ]\n",
    "\n",
    "for fname in datz:\n",
    "    parse_json_file_to_csv(fname)\n",
    "    print(\"******** FINISHED - \", fname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "abid=\"https://api.elsevier.com/content/abstract/scopus_id/85098927137\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.strptime(\"2021-09-01\", \"%Y-%m-%d\").year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(int('fundus' in \"The quick brown\".lower() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
