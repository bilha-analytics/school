{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "friendly-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import logging\n",
    "from datetime import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "biblical-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyscopus import Scopus\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neutral-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_API_KEY = '6f08887e7863302c0431a454c96fb54c'\n",
    "headerz_api={'Accept':'application/json', 'X-ELS-APIKey': MY_API_KEY}\n",
    "\n",
    "class TargetObject: \n",
    "    def __init__(self, urlz, headerz, parser_xtra ):\n",
    "        self.urlz = urlz\n",
    "        self.headerz = headerz\n",
    "        self.parser_xtra = parser_xtra\n",
    "        print(\"Target: \", self.urlz)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exterior-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus = Scopus(MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "interior-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                  Sat, 10 Apr 2021 03:04:05 GMT\n",
      "Content-Type          application/json;charset=UTF-8\n",
      "Transfer-Encoding     chunked\n",
      "Connection            keep-alive\n",
      "allow                 GET\n",
      "Content-Encoding      gzip\n",
      "Vary                  Accept-Encoding, Origin\n",
      "X-ELS-APIKey          6f08887e7863302c0431a454c96fb54c\n",
      "X-ELS-ReqId           bf388d067f3600d8\n",
      "X-ELS-ResourceVersion  default\n",
      "X-ELS-Status          OK\n",
      "X-ELS-TransId         2c8b40325ea4934f\n",
      "X-RateLimit-Limit     20000\n",
      "X-RateLimit-Remaining  20000\n",
      "X-RateLimit-Reset     1618628644000\n",
      "CF-Cache-Status       DYNAMIC\n",
      "cf-request-id         095b55a03a00001b5a699d4000000001\n",
      "Expect-CT             max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"\n",
      "Server                cloudflare\n",
      "CF-RAY                63d8bee05a4a1b5a-NBO\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(\"http://api.elsevier.com/content/search/scopus?query=fundus&count=25\", #af-id(60032114)+OR+af-id(60022265)\n",
    "                    headers={'Accept':'application/json',\n",
    "                             'X-RateLimit-Limit':'1',                             \n",
    "                             'X-ELS-APIKey': MY_API_KEY})\n",
    "\n",
    "\n",
    "for k, v in resp.headers.items():\n",
    "    print(\"{:20}  {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "smooth-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(query, fname):\n",
    "    def save_to_file(data, fname):\n",
    "        with open(fname, 'a', encoding='utf-8') as f:\n",
    "            json.dump( data, f, ensure_ascii=False, indent=4)\n",
    "            f.write(\",\\n\")\n",
    "\n",
    "    pg = 0\n",
    "    TOTAL_ITEMS = 0\n",
    "    PAGEZ = 10 \n",
    "    PG_MAX = 25\n",
    "    # while pg is not None:\n",
    "    # for pg in range(PAGEZ): \n",
    "    while pg < PAGEZ:\n",
    "        ## 1. get page    \n",
    "        resp = requests.get(f\"http://api.elsevier.com/content/search/scopus?query={query}&count={PG_MAX}&start={pg*PG_MAX}\", #af-id(60032114)+OR+af-id(60022265)\n",
    "                        headers={'Accept':'application/json',\n",
    "                                 'X-ELS-APIKey': MY_API_KEY})\n",
    "        REZ = resp.headers['X-ELS-Status']\n",
    "        if REZ.strip() != 'OK':\n",
    "            print( f\"*****{REZ}\")\n",
    "            break\n",
    "        pg += 1\n",
    "        \n",
    "        ## 2. get next page id  and content object \n",
    "        for k, v in resp.json().items(): \n",
    "            for r, v in resp.json()['search-results'].items():\n",
    "                if r == 'opensearch:totalResults' and TOTAL_ITEMS == 0:\n",
    "                    TOTAL_ITEMS = int( v )\n",
    "                    PAGEZ = TOTAL_ITEMS//PG_MAX\n",
    "                    mod = TOTAL_ITEMS%PG_MAX\n",
    "                    PAGEZ += 0 if mod == 0 else 1 \n",
    "                    print( \"*** TOTAL_ITEMS: \", TOTAL_ITEMS, \", << \", PAGEZ) \n",
    "                    \n",
    "                if r == 'entry':        \n",
    "                    ## save to file\n",
    "                    save_to_file(v, f\"0ea_{PAGEZ}__{fname}\")\n",
    "                    print(f\"PG: {pg} dumped to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wound-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_file_to_csv(fname): \n",
    "    keyz = [ 'prism:url', \n",
    "             \"dc:identifier\", \"dc:title\", \"prism:publicationName\",\n",
    "             \"prism:coverDate\", \"citedby-count\", \n",
    "            ]\n",
    "    affilz = \"affiliation\" ## sub = affilname, affiliation-city, affiliation-country\n",
    "    first_affilz_country = \"country_1\"\n",
    "    first_affilz_org = \"institution_1\"\n",
    "    \n",
    "    has_fundus_in_title = \"fundus_in_title\"\n",
    "    pub_year = \"pub_year\"\n",
    "    \n",
    "    abhref = 'abstract'\n",
    "    \n",
    "    def parse_elsevier_json_item(item):    \n",
    "        \n",
    "        def get_pub_year(pub_date):\n",
    "            return str( datetime.strptime(pub_date, \"%Y-%m-%d\").year )\n",
    "        \n",
    "        def has_fundus(titl):\n",
    "            return str( int( 'fundus' in titl.lower() ) )\n",
    "        \n",
    "        O_ = []\n",
    "        ## 1. everything top level \n",
    "        for k in keyz:\n",
    "            O_.append( item.get(k, '')) #item[k] ) \n",
    "        \n",
    "        ## 2. parse affiliation details and join org,city,country\n",
    "        k_affz = ['affilname', 'affiliation-city', 'affiliation-country']\n",
    "        A_ = []\n",
    "        I_ = '-'\n",
    "        C_ = '-'\n",
    "        ia = 0 \n",
    "        affitem = item.get(\"affiliation\", None)\n",
    "        if affitem:\n",
    "            for aff in affitem:\n",
    "                if aff:\n",
    "                    a_ = []\n",
    "                    for k in k_affz:\n",
    "                        x = aff.get(k, \"-\") \n",
    "                        a_.append( x if x is not None else \"-\")\n",
    "#                     print( a_ )\n",
    "                    A_.append(\", \".join(a_) )         \n",
    "                    if ia == 0:\n",
    "                        C_ = a_[-1]  \n",
    "                        I_ = a_[0]\n",
    "                        ia = 100 \n",
    "            O_.append( \"++\".join(A_) )\n",
    "        else:\n",
    "            O_.append( \"-\" )\n",
    "        O_.append( I_ )\n",
    "        O_.append( C_ )\n",
    "        \n",
    "        ## 3. preprocs\n",
    "        O_.append( has_fundus( item.get(\"dc:title\", \"\") ) )\n",
    "        O_.append( get_pub_year(item.get(\"prism:coverDate\", \"\")))\n",
    "        \n",
    "        ## 4. parse abstract url \n",
    "        abstract = \"link\" # @ref = \"self\", \"@href\" \n",
    "        for abl in item[abstract]:\n",
    "            if abl['@ref'] == 'self':\n",
    "                O_.append( abl['@href'] ) \n",
    "        \n",
    "        return O_ \n",
    "    \n",
    "        \n",
    "    def fetch_abstract(ahref):\n",
    "        pass \n",
    "    \n",
    "    to_csv = []\n",
    "    with open( f'{fname}', 'r') as fd:\n",
    "        pagez = json.load( fd)\n",
    "#         print( type(pagez), len(pagez) )\n",
    "        for page in pagez:\n",
    "            for item in page:\n",
    "                try:\n",
    "                    to_csv.append( parse_elsevier_json_item(item) )\n",
    "                except:\n",
    "                    print(f\"failed at item: {str(item)}\")\n",
    "            \n",
    "    \n",
    "    def dump_csv_row(rec):\n",
    "#         print(type(rec), len(rec), rec)\n",
    "        fd.write( \"\\t\".join( rec) )\n",
    "        fd.write(\"\\n\")\n",
    "        \n",
    "    fcsv = fname.split(\".\")[0] \n",
    "    with open( f\"{fcsv}.csv\", 'w') as fd:\n",
    "        headz = keyz + [affilz, first_affilz_org, first_affilz_country, \n",
    "                        has_fundus_in_title, pub_year, \n",
    "                        abhref] \n",
    "        dump_csv_row(headz)\n",
    "        for rec in to_csv:\n",
    "            dump_csv_row(rec)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-national",
   "metadata": {},
   "source": [
    "# Search Strategies\n",
    "\n",
    "[From Scopus Webinar](https://blog.scopus.com/posts/6-simple-search-tips-lessons-learned-from-the-scopus-webinar)\n",
    "- Phareses in quotes (loose phrase) else treated as AND\n",
    "- TITLE-ABS-KEY is default search field --> \n",
    "- Auto thingies\n",
    "    - case insensitive\n",
    "    - accented characters - with or without \n",
    "    - lemmatization --> conjugations etc <-- EXACT PHRASE marker if don't want this\n",
    "    - equivalents @ terms or symbols\n",
    "    - punctuations are ignored\n",
    "    - stopwords are exluded\n",
    "    - override with exact phase --> enclose in braces {}\n",
    "- Proximity operators to find words near each other --> e.g. preceding Pre/n, within W/n, <-- E.g. traditional Pre/2 features to capture 'in between words' or W/2 if id doesn't matter which words comes first\n",
    "- Wildcards - in any word or loose phrase e.g. spelling variations. `?` or `*` == any single character AND n >= 0 respectively \n",
    "- [Scopus help files](https://blog.scopus.com/topics/tips-and-tricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "young-center",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TOTAL_ITEMS:  14 , <<  1\n",
      "PG: 1 dumped to file\n",
      "*** TOTAL_ITEMS:  3479 , <<  140\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "PG: 27 dumped to file\n",
      "PG: 28 dumped to file\n",
      "PG: 29 dumped to file\n",
      "PG: 30 dumped to file\n",
      "PG: 31 dumped to file\n",
      "PG: 32 dumped to file\n",
      "PG: 33 dumped to file\n",
      "PG: 34 dumped to file\n",
      "PG: 35 dumped to file\n",
      "PG: 36 dumped to file\n",
      "PG: 37 dumped to file\n",
      "PG: 38 dumped to file\n",
      "PG: 39 dumped to file\n",
      "PG: 40 dumped to file\n",
      "PG: 41 dumped to file\n",
      "PG: 42 dumped to file\n",
      "PG: 43 dumped to file\n",
      "PG: 44 dumped to file\n",
      "PG: 45 dumped to file\n",
      "PG: 46 dumped to file\n",
      "PG: 47 dumped to file\n",
      "PG: 48 dumped to file\n",
      "PG: 49 dumped to file\n",
      "PG: 50 dumped to file\n",
      "PG: 51 dumped to file\n",
      "PG: 52 dumped to file\n",
      "PG: 53 dumped to file\n",
      "PG: 54 dumped to file\n",
      "PG: 55 dumped to file\n",
      "PG: 56 dumped to file\n",
      "PG: 57 dumped to file\n",
      "PG: 58 dumped to file\n",
      "PG: 59 dumped to file\n",
      "PG: 60 dumped to file\n",
      "PG: 61 dumped to file\n",
      "PG: 62 dumped to file\n",
      "PG: 63 dumped to file\n",
      "PG: 64 dumped to file\n",
      "PG: 65 dumped to file\n",
      "PG: 66 dumped to file\n",
      "PG: 67 dumped to file\n",
      "PG: 68 dumped to file\n",
      "PG: 69 dumped to file\n",
      "PG: 70 dumped to file\n",
      "PG: 71 dumped to file\n",
      "PG: 72 dumped to file\n",
      "PG: 73 dumped to file\n",
      "PG: 74 dumped to file\n",
      "PG: 75 dumped to file\n",
      "PG: 76 dumped to file\n",
      "PG: 77 dumped to file\n",
      "PG: 78 dumped to file\n",
      "PG: 79 dumped to file\n",
      "PG: 80 dumped to file\n",
      "PG: 81 dumped to file\n",
      "PG: 82 dumped to file\n",
      "PG: 83 dumped to file\n",
      "PG: 84 dumped to file\n",
      "PG: 85 dumped to file\n",
      "PG: 86 dumped to file\n",
      "PG: 87 dumped to file\n",
      "PG: 88 dumped to file\n",
      "PG: 89 dumped to file\n",
      "PG: 90 dumped to file\n",
      "PG: 91 dumped to file\n",
      "PG: 92 dumped to file\n",
      "PG: 93 dumped to file\n",
      "PG: 94 dumped to file\n",
      "PG: 95 dumped to file\n",
      "PG: 96 dumped to file\n",
      "PG: 97 dumped to file\n",
      "PG: 98 dumped to file\n",
      "PG: 99 dumped to file\n",
      "PG: 100 dumped to file\n",
      "PG: 101 dumped to file\n",
      "PG: 102 dumped to file\n",
      "PG: 103 dumped to file\n",
      "PG: 104 dumped to file\n",
      "PG: 105 dumped to file\n",
      "PG: 106 dumped to file\n",
      "PG: 107 dumped to file\n",
      "PG: 108 dumped to file\n",
      "PG: 109 dumped to file\n",
      "PG: 110 dumped to file\n",
      "PG: 111 dumped to file\n",
      "PG: 112 dumped to file\n",
      "PG: 113 dumped to file\n",
      "PG: 114 dumped to file\n",
      "PG: 115 dumped to file\n",
      "PG: 116 dumped to file\n",
      "PG: 117 dumped to file\n",
      "PG: 118 dumped to file\n",
      "PG: 119 dumped to file\n",
      "PG: 120 dumped to file\n",
      "PG: 121 dumped to file\n",
      "PG: 122 dumped to file\n",
      "PG: 123 dumped to file\n",
      "PG: 124 dumped to file\n",
      "PG: 125 dumped to file\n",
      "PG: 126 dumped to file\n",
      "PG: 127 dumped to file\n",
      "PG: 128 dumped to file\n",
      "PG: 129 dumped to file\n",
      "PG: 130 dumped to file\n",
      "PG: 131 dumped to file\n",
      "PG: 132 dumped to file\n",
      "PG: 133 dumped to file\n",
      "PG: 134 dumped to file\n",
      "PG: 135 dumped to file\n",
      "PG: 136 dumped to file\n",
      "PG: 137 dumped to file\n",
      "PG: 138 dumped to file\n",
      "PG: 139 dumped to file\n",
      "PG: 140 dumped to file\n",
      "*** TOTAL_ITEMS:  1622 , <<  65\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "PG: 27 dumped to file\n",
      "PG: 28 dumped to file\n",
      "PG: 29 dumped to file\n",
      "PG: 30 dumped to file\n",
      "PG: 31 dumped to file\n",
      "PG: 32 dumped to file\n",
      "PG: 33 dumped to file\n",
      "PG: 34 dumped to file\n",
      "PG: 35 dumped to file\n",
      "PG: 36 dumped to file\n",
      "PG: 37 dumped to file\n",
      "PG: 38 dumped to file\n",
      "PG: 39 dumped to file\n",
      "PG: 40 dumped to file\n",
      "PG: 41 dumped to file\n",
      "PG: 42 dumped to file\n",
      "PG: 43 dumped to file\n",
      "PG: 44 dumped to file\n",
      "PG: 45 dumped to file\n",
      "PG: 46 dumped to file\n",
      "PG: 47 dumped to file\n",
      "PG: 48 dumped to file\n",
      "PG: 49 dumped to file\n",
      "PG: 50 dumped to file\n",
      "PG: 51 dumped to file\n",
      "PG: 52 dumped to file\n",
      "PG: 53 dumped to file\n",
      "PG: 54 dumped to file\n",
      "PG: 55 dumped to file\n",
      "PG: 56 dumped to file\n",
      "PG: 57 dumped to file\n",
      "PG: 58 dumped to file\n",
      "PG: 59 dumped to file\n",
      "PG: 60 dumped to file\n",
      "PG: 61 dumped to file\n",
      "PG: 62 dumped to file\n",
      "PG: 63 dumped to file\n",
      "PG: 64 dumped to file\n",
      "PG: 65 dumped to file\n",
      "*** TOTAL_ITEMS:  3710 , <<  149\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "PG: 27 dumped to file\n",
      "PG: 28 dumped to file\n",
      "PG: 29 dumped to file\n",
      "PG: 30 dumped to file\n",
      "PG: 31 dumped to file\n",
      "PG: 32 dumped to file\n",
      "PG: 33 dumped to file\n",
      "PG: 34 dumped to file\n",
      "PG: 35 dumped to file\n",
      "PG: 36 dumped to file\n",
      "PG: 37 dumped to file\n",
      "PG: 38 dumped to file\n",
      "PG: 39 dumped to file\n",
      "PG: 40 dumped to file\n",
      "PG: 41 dumped to file\n",
      "PG: 42 dumped to file\n",
      "PG: 43 dumped to file\n",
      "PG: 44 dumped to file\n",
      "PG: 45 dumped to file\n",
      "PG: 46 dumped to file\n",
      "PG: 47 dumped to file\n",
      "PG: 48 dumped to file\n",
      "PG: 49 dumped to file\n",
      "PG: 50 dumped to file\n",
      "PG: 51 dumped to file\n",
      "PG: 52 dumped to file\n",
      "PG: 53 dumped to file\n",
      "PG: 54 dumped to file\n",
      "PG: 55 dumped to file\n",
      "PG: 56 dumped to file\n",
      "PG: 57 dumped to file\n",
      "PG: 58 dumped to file\n",
      "PG: 59 dumped to file\n",
      "PG: 60 dumped to file\n",
      "PG: 61 dumped to file\n",
      "PG: 62 dumped to file\n",
      "PG: 63 dumped to file\n",
      "PG: 64 dumped to file\n",
      "PG: 65 dumped to file\n",
      "PG: 66 dumped to file\n",
      "PG: 67 dumped to file\n",
      "PG: 68 dumped to file\n",
      "PG: 69 dumped to file\n",
      "PG: 70 dumped to file\n",
      "PG: 71 dumped to file\n",
      "PG: 72 dumped to file\n",
      "PG: 73 dumped to file\n",
      "PG: 74 dumped to file\n",
      "PG: 75 dumped to file\n",
      "PG: 76 dumped to file\n",
      "PG: 77 dumped to file\n",
      "PG: 78 dumped to file\n",
      "PG: 79 dumped to file\n",
      "PG: 80 dumped to file\n",
      "PG: 81 dumped to file\n",
      "PG: 82 dumped to file\n",
      "PG: 83 dumped to file\n",
      "PG: 84 dumped to file\n",
      "PG: 85 dumped to file\n",
      "PG: 86 dumped to file\n",
      "PG: 87 dumped to file\n",
      "PG: 88 dumped to file\n",
      "PG: 89 dumped to file\n",
      "PG: 90 dumped to file\n",
      "PG: 91 dumped to file\n",
      "PG: 92 dumped to file\n",
      "PG: 93 dumped to file\n",
      "PG: 94 dumped to file\n",
      "PG: 95 dumped to file\n",
      "PG: 96 dumped to file\n",
      "PG: 97 dumped to file\n",
      "PG: 98 dumped to file\n",
      "PG: 99 dumped to file\n",
      "PG: 100 dumped to file\n",
      "PG: 101 dumped to file\n",
      "PG: 102 dumped to file\n",
      "PG: 103 dumped to file\n",
      "PG: 104 dumped to file\n",
      "PG: 105 dumped to file\n",
      "PG: 106 dumped to file\n",
      "PG: 107 dumped to file\n",
      "PG: 108 dumped to file\n",
      "PG: 109 dumped to file\n",
      "PG: 110 dumped to file\n",
      "PG: 111 dumped to file\n",
      "PG: 112 dumped to file\n",
      "PG: 113 dumped to file\n",
      "PG: 114 dumped to file\n",
      "PG: 115 dumped to file\n",
      "PG: 116 dumped to file\n",
      "PG: 117 dumped to file\n",
      "PG: 118 dumped to file\n",
      "PG: 119 dumped to file\n",
      "PG: 120 dumped to file\n",
      "PG: 121 dumped to file\n",
      "PG: 122 dumped to file\n",
      "PG: 123 dumped to file\n",
      "PG: 124 dumped to file\n",
      "PG: 125 dumped to file\n",
      "PG: 126 dumped to file\n",
      "PG: 127 dumped to file\n",
      "PG: 128 dumped to file\n",
      "PG: 129 dumped to file\n",
      "PG: 130 dumped to file\n",
      "PG: 131 dumped to file\n",
      "PG: 132 dumped to file\n",
      "PG: 133 dumped to file\n",
      "PG: 134 dumped to file\n",
      "PG: 135 dumped to file\n",
      "PG: 136 dumped to file\n",
      "PG: 137 dumped to file\n",
      "PG: 138 dumped to file\n",
      "PG: 139 dumped to file\n",
      "PG: 140 dumped to file\n",
      "PG: 141 dumped to file\n",
      "PG: 142 dumped to file\n",
      "PG: 143 dumped to file\n",
      "PG: 144 dumped to file\n",
      "PG: 145 dumped to file\n",
      "PG: 146 dumped to file\n",
      "PG: 147 dumped to file\n",
      "PG: 148 dumped to file\n",
      "PG: 149 dumped to file\n",
      "*****NO_SEARCH_RESULTS(Result set was empty)\n"
     ]
    }
   ],
   "source": [
    "searchez = [ (\"fundus 'traditional W/2 analysis' 'deep W/2 learning'\", 'fundus_hc_dl.json'),\n",
    "           (\"medical 'image W/2 preprocessing'\", \"medical_img_preproc.json\"),\n",
    "           (\"image 'fuse W/2 features'\", 'fuse_hc_dl.json'),\n",
    "           (\"image 'combine W/2 features'\", 'fuse2_hc_dl.json'),\n",
    "           (\"image 'qualtity W/2 measurement'\", 'quality.json')]\n",
    "\n",
    "\n",
    "for q, f in searchez:\n",
    "    run_search(q, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fossil-cabinet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TOTAL_ITEMS:  50 , <<  2\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "*** TOTAL_ITEMS:  88 , <<  4\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "*** TOTAL_ITEMS:  1114 , <<  45\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "PG: 27 dumped to file\n",
      "PG: 28 dumped to file\n",
      "PG: 29 dumped to file\n",
      "PG: 30 dumped to file\n",
      "PG: 31 dumped to file\n",
      "PG: 32 dumped to file\n",
      "PG: 33 dumped to file\n",
      "PG: 34 dumped to file\n",
      "PG: 35 dumped to file\n",
      "PG: 36 dumped to file\n",
      "PG: 37 dumped to file\n",
      "PG: 38 dumped to file\n",
      "PG: 39 dumped to file\n",
      "PG: 40 dumped to file\n",
      "PG: 41 dumped to file\n",
      "PG: 42 dumped to file\n",
      "PG: 43 dumped to file\n",
      "PG: 44 dumped to file\n",
      "PG: 45 dumped to file\n",
      "*** TOTAL_ITEMS:  635 , <<  26\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "*** TOTAL_ITEMS:  469 , <<  19\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "*** TOTAL_ITEMS:  2300 , <<  92\n",
      "PG: 1 dumped to file\n",
      "PG: 2 dumped to file\n",
      "PG: 3 dumped to file\n",
      "PG: 4 dumped to file\n",
      "PG: 5 dumped to file\n",
      "PG: 6 dumped to file\n",
      "PG: 7 dumped to file\n",
      "PG: 8 dumped to file\n",
      "PG: 9 dumped to file\n",
      "PG: 10 dumped to file\n",
      "PG: 11 dumped to file\n",
      "PG: 12 dumped to file\n",
      "PG: 13 dumped to file\n",
      "PG: 14 dumped to file\n",
      "PG: 15 dumped to file\n",
      "PG: 16 dumped to file\n",
      "PG: 17 dumped to file\n",
      "PG: 18 dumped to file\n",
      "PG: 19 dumped to file\n",
      "PG: 20 dumped to file\n",
      "PG: 21 dumped to file\n",
      "PG: 22 dumped to file\n",
      "PG: 23 dumped to file\n",
      "PG: 24 dumped to file\n",
      "PG: 25 dumped to file\n",
      "PG: 26 dumped to file\n",
      "PG: 27 dumped to file\n",
      "PG: 28 dumped to file\n",
      "PG: 29 dumped to file\n",
      "PG: 30 dumped to file\n",
      "PG: 31 dumped to file\n",
      "PG: 32 dumped to file\n",
      "PG: 33 dumped to file\n",
      "PG: 34 dumped to file\n",
      "PG: 35 dumped to file\n",
      "PG: 36 dumped to file\n",
      "PG: 37 dumped to file\n",
      "PG: 38 dumped to file\n",
      "PG: 39 dumped to file\n",
      "PG: 40 dumped to file\n",
      "PG: 41 dumped to file\n",
      "PG: 42 dumped to file\n",
      "PG: 43 dumped to file\n",
      "PG: 44 dumped to file\n",
      "PG: 45 dumped to file\n",
      "PG: 46 dumped to file\n",
      "PG: 47 dumped to file\n",
      "PG: 48 dumped to file\n",
      "PG: 49 dumped to file\n",
      "PG: 50 dumped to file\n",
      "PG: 51 dumped to file\n",
      "PG: 52 dumped to file\n",
      "PG: 53 dumped to file\n",
      "PG: 54 dumped to file\n",
      "PG: 55 dumped to file\n",
      "PG: 56 dumped to file\n",
      "PG: 57 dumped to file\n",
      "PG: 58 dumped to file\n",
      "PG: 59 dumped to file\n",
      "PG: 60 dumped to file\n",
      "PG: 61 dumped to file\n",
      "PG: 62 dumped to file\n",
      "PG: 63 dumped to file\n",
      "PG: 64 dumped to file\n",
      "PG: 65 dumped to file\n",
      "PG: 66 dumped to file\n",
      "PG: 67 dumped to file\n",
      "PG: 68 dumped to file\n",
      "PG: 69 dumped to file\n",
      "PG: 70 dumped to file\n",
      "PG: 71 dumped to file\n",
      "PG: 72 dumped to file\n",
      "PG: 73 dumped to file\n",
      "PG: 74 dumped to file\n",
      "PG: 75 dumped to file\n",
      "PG: 76 dumped to file\n",
      "PG: 77 dumped to file\n",
      "PG: 78 dumped to file\n",
      "PG: 79 dumped to file\n",
      "PG: 80 dumped to file\n",
      "PG: 81 dumped to file\n",
      "PG: 82 dumped to file\n",
      "PG: 83 dumped to file\n",
      "PG: 84 dumped to file\n",
      "PG: 85 dumped to file\n",
      "PG: 86 dumped to file\n",
      "PG: 87 dumped to file\n",
      "PG: 88 dumped to file\n",
      "PG: 89 dumped to file\n",
      "PG: 90 dumped to file\n",
      "PG: 91 dumped to file\n",
      "PG: 92 dumped to file\n"
     ]
    }
   ],
   "source": [
    "searchez = [ (\"fundus 'traditional W/2 features'\", 'fundus-2_hc_dl.json'),\n",
    "             (\"fundus 'handcrafted W/2 features'\", 'fundus-3_hc_dl.json'), \n",
    "             (\"fundus 'deep W/2 features'\", 'fundus-4_hc_dl.json'),  \n",
    "           (\"fundus 'multi W/2 learning'\", 'fundus-multi-1_hc_dl.json'), \n",
    "           (\"fundus 'multiple W/2 disease'\", 'fundus-multi-2_hc_dl.json'),\n",
    "           (\"image 'handcrafted W/2 features'\", 'quality.json')]\n",
    "\n",
    "\n",
    "for q, f in searchez:\n",
    "    run_search(q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "suspected-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** FINISHED -  0ea_140__medical_img_preproc.json\n",
      "******** FINISHED -  0ea_149__fuse2_hc_dl.json\n",
      "******** FINISHED -  0ea_19__fundus-multi-2_hc_dl.json\n",
      "******** FINISHED -  0ea_1__fundus_hc_dl.json\n",
      "******** FINISHED -  0ea_26__fundus-multi-1_hc_dl.json\n",
      "******** FINISHED -  0ea_2__fundus-2_hc_dl.json\n",
      "******** FINISHED -  0ea_45__fundus-4_hc_dl.json\n",
      "******** FINISHED -  0ea_4__fundus-3_hc_dl.json\n",
      "******** FINISHED -  0ea_65__fuse_hc_dl.json\n",
      "******** FINISHED -  0ea_92__quality.json\n"
     ]
    }
   ],
   "source": [
    "# datz = ['0ea_4__fuse_hc_dl', \n",
    "#         '0ea_5__fundus_hc_dl', \n",
    "#         '0ea_642__medical_img_preproc', \n",
    "#         '0ea_3926__fundus'\n",
    "#        ]\n",
    "\n",
    "datz = glob.glob(\"*.json\")\n",
    "\n",
    "for fname in datz:\n",
    "    parse_json_file_to_csv(fname)\n",
    "    print(\"******** FINISHED - \", fname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "coastal-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(ls_fnamez, ls_category_labelz, mname):\n",
    "    dfz = [pd.read_csv(f\"0ea_{f}.csv\", sep='\\t', ) for f in ls_fnamez ]\n",
    "    print( len(dfz ) )\n",
    "    print( len(dfz[0]), dfz[0].columns ) \n",
    "    #add_category = lambda x: x[0]['ctype'] = x[1]\n",
    "    def add_category(df, lbl):\n",
    "        df['ctype'] = lbl \n",
    "        return df\n",
    "    dfz = [ add_category(d, l) for d, l in zip(dfz, ls_category_labelz) ]\n",
    "    df = pd.concat( dfz , ignore_index=True)\n",
    "    df.to_csv(f\"{mname}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "tender-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergez = [(['1__fundus_hc_dl', '2__fundus-2_hc_dl', '4__fundus-3_hc_dl', '45__fundus-4_hc_dl',\n",
    "                \"140__medical_img_preproc\",  '92__quality'],\n",
    "           ['Fundus HC or DL', 'FUndus HC', 'Fundus HC', 'Fundus DL', \n",
    "                'IMG Preproc', 'IMG HC-Qy'], \n",
    "           'funduz_hc_dl_ALL'),\n",
    "         \n",
    "          (['65__fuse_hc_dl', '149__fuse2_hc_dl'],\n",
    "           ['IMG fusion', 'IMG fusion'],\n",
    "           'general_fuse'),\n",
    "          \n",
    "          (['26__fundus-multi-1_hc_dl', '19__fundus-multi-2_hc_dl'], \n",
    "           ['Fundus multi-disease', 'Fundus multi-disease'],\n",
    "           \"funduz_multitask\")\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "trying-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGING:  ['1__fundus_hc_dl', '2__fundus-2_hc_dl', '4__fundus-3_hc_dl', '45__fundus-4_hc_dl', '140__medical_img_preproc', '92__quality']\n",
      "6\n",
      "14 Index(['prism:url', 'dc:identifier', 'dc:title', 'prism:publicationName',\n",
      "       'prism:coverDate', 'citedby-count', 'affiliation', 'institution_1',\n",
      "       'country_1', 'fundus_in_title', 'pub_year', 'abstract'],\n",
      "      dtype='object')\n",
      "MERGING:  ['65__fuse_hc_dl', '149__fuse2_hc_dl']\n",
      "2\n",
      "1622 Index(['prism:url', 'dc:identifier', 'dc:title', 'prism:publicationName',\n",
      "       'prism:coverDate', 'citedby-count', 'affiliation', 'institution_1',\n",
      "       'country_1', 'fundus_in_title', 'pub_year', 'abstract'],\n",
      "      dtype='object')\n",
      "MERGING:  ['26__fundus-multi-1_hc_dl', '19__fundus-multi-2_hc_dl']\n",
      "2\n",
      "635 Index(['prism:url', 'dc:identifier', 'dc:title', 'prism:publicationName',\n",
      "       'prism:coverDate', 'citedby-count', 'affiliation', 'institution_1',\n",
      "       'country_1', 'fundus_in_title', 'pub_year', 'abstract'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for fz, lb, n in mergez:\n",
    "    print(\"MERGING: \", fz)\n",
    "    merge_csv_files( fz, lb, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-vegetation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
